{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "075468bb",
   "metadata": {},
   "source": [
    "# Explore Classifier Results\n",
    "\n",
    "This notebook explores the results from each of our sameness classification methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0164eaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from vis import describe_results, plot_confidence_intervals\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9452195f",
   "metadata": {},
   "source": [
    "Look through a table of all results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737369ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all CSV file paths in data/test_results/\n",
    "csv_files = glob.glob('data/test-results/*.csv')\n",
    "\n",
    "# Read each CSV file into a DataFrame and store in a list\n",
    "RESULT_DFS = [pd.read_csv(file) for file in csv_files]\n",
    "\n",
    "# Extract model names from the filenames\n",
    "MODELS_USED = [file.split('/')[-1].replace('_eval.csv', '') for file in csv_files]\n",
    "\n",
    "# Calculate metrics\n",
    "metrics_df = describe_results(\n",
    "    dfs=RESULT_DFS,\n",
    "    models_used=MODELS_USED, \n",
    "    y_true_col='classification', \n",
    "    y_pred_col='pred',\n",
    "    even_classes=True\n",
    ")\n",
    "\n",
    "# Save the results to a CSV\n",
    "metrics_df.to_csv('data/outputs/results.csv')\n",
    "\n",
    "metrics_df.sort_values(by='model_used', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31becda",
   "metadata": {},
   "source": [
    "Generate confidence interval plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81d4e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_result_paths = [\n",
    "    'gpt_4o_no_examples_eval.csv',\n",
    "    'tf_idf_similarity_eval.csv',\n",
    "    'gpt_4o_mini_generated_examples_eval.csv',\n",
    "    'gpt_4o_mini_overfit_with_original.csv',\n",
    "    'gpt_35_turbo_generated_examples_eval.csv',\n",
    "    'gpt_4o_mini_with_pseudo_examples_eval.csv',\n",
    "    'gpt_4o_mini_with_examples_eval.csv',\n",
    "    'second_half_similarity_eval.csv',\n",
    "    'gpt_35_turbo_no_examples_eval.csv',\n",
    "    'gpt_4o_mini_no_examples_eval.csv',\n",
    "    'gpt_4o_with_pseudo_examples_eval.csv',\n",
    "    'gpt_4o_with_examples_eval.csv',\n",
    "    'semantic_similarity_eval.csv',\n",
    "    'gpt_35_turbo_with_pseudo_examples_eval.csv',\n",
    "    'gpt_4o_generated_examples_eval.csv',\n",
    "    'gpt_4o_overfit_pure_eval.csv',\n",
    "    'gpt_4o_mini_overfit_pure_eval.csv',\n",
    "    'gpt_35_turbo_with_examples_eval.csv'\n",
    "]\n",
    "\n",
    "base_result_paths = [\n",
    "    'gpt_4o_no_examples_eval.csv',\n",
    "    'tf_idf_similarity_eval.csv',\n",
    "    'second_half_similarity_eval.csv',\n",
    "    'gpt_35_turbo_no_examples_eval.csv',\n",
    "    'gpt_4o_mini_no_examples_eval.csv',\n",
    "    'semantic_similarity_eval.csv',\n",
    "]\n",
    "\n",
    "gpt_4o_paths = [\n",
    "    'gpt_4o_no_examples_eval.csv',\n",
    "    'gpt_4o_with_pseudo_examples_eval.csv',\n",
    "    'gpt_4o_with_examples_eval.csv',\n",
    "    'gpt_4o_generated_examples_eval.csv',\n",
    "    'gpt_4o_overfit_pure_eval.csv',\n",
    "]\n",
    "\n",
    "# Example usage\n",
    "data_dir = 'data/test-results copy'\n",
    "plot_confidence_intervals(\n",
    "    csv_paths=base_result_paths, \n",
    "    output_path='imgs/base-results', \n",
    "    title='Base',\n",
    "    confidence=0.95)\n",
    "\n",
    "plot_confidence_intervals(\n",
    "    csv_paths=gpt_4o_paths, \n",
    "    output_path='imgs/gpt-4o-results', \n",
    "    title='gpt-4o',\n",
    "    confidence=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4777a6",
   "metadata": {},
   "source": [
    "## What did the models get wrong?\n",
    "Use the following cells to explore the examples that each model struggles with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558a00eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {model_name: result for model_name, result in zip(MODELS_USED, RESULT_DFS)}\n",
    "MODELS_USED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912d378a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = results['gpt_4o_generated_examples']\n",
    "wrong = df[df['classification'] != df['pred']]\n",
    "print(wrong.shape)\n",
    "wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae35c90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.13.0 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "bc72e7bb5a5c6478fda78e4993a878b385c4536c68455efd83492604af2a5a7a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

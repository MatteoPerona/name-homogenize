{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "075468bb",
   "metadata": {},
   "source": [
    "# Explore Classifier Results\n",
    "\n",
    "This notebook explores the results from each of our sameness classification methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0164eaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from vis import describe_results, plot_confidence_intervals\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737369ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all CSV file paths in data/test_results/\n",
    "csv_files = glob.glob('data/test-results/*.csv')\n",
    "\n",
    "# Read each CSV file into a DataFrame and store in a list\n",
    "RESULT_DFS = [pd.read_csv(file) for file in csv_files]\n",
    "\n",
    "# Extract model names from the filenames\n",
    "MODELS_USED = [file.split('/')[-1].replace('_eval.csv', '') for file in csv_files]\n",
    "\n",
    "# Calculate metrics\n",
    "metrics_df = describe_results(\n",
    "    dfs=RESULT_DFS,\n",
    "    models_used=MODELS_USED, \n",
    "    y_true_col='classification', \n",
    "    y_pred_col='pred',\n",
    "    even_classes=True\n",
    ")\n",
    "\n",
    "# Save the results to a CSV\n",
    "metrics_df.to_csv('data/outputs/results.csv')\n",
    "\n",
    "metrics_df.sort_values(by='model_used', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81d4e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_result_paths = [\n",
    "    'gpt_4o_no_examples_eval.csv',\n",
    "    'tf_idf_similarity_eval.csv',\n",
    "    'gpt_4o_mini_generated_examples_eval.csv',\n",
    "    'gpt_4o_mini_overfit_with_original.csv',\n",
    "    'gpt_35_turbo_generated_examples_eval.csv',\n",
    "    'gpt_4o_mini_with_pseudo_examples_eval.csv',\n",
    "    'gpt_4o_mini_with_examples_eval.csv',\n",
    "    'second_half_similarity_eval.csv',\n",
    "    'gpt_35_turbo_no_examples_eval.csv',\n",
    "    'gpt_4o_mini_no_examples_eval.csv',\n",
    "    'gpt_4o_with_pseudo_examples_eval.csv',\n",
    "    'gpt_4o_with_examples_eval.csv',\n",
    "    'semantic_similarity_eval.csv',\n",
    "    'gpt_35_turbo_with_pseudo_examples_eval.csv',\n",
    "    'gpt_4o_generated_examples_eval.csv',\n",
    "    'gpt_4o_overfit_pure_eval.csv',\n",
    "    'gpt_4o_mini_overfit_pure_eval.csv',\n",
    "    'gpt_35_turbo_with_examples_eval.csv'\n",
    "]\n",
    "\n",
    "base_result_paths = [\n",
    "    'gpt_4o_no_examples_eval.csv',\n",
    "    'tf_idf_similarity_eval.csv',\n",
    "    'second_half_similarity_eval.csv',\n",
    "    'gpt_35_turbo_no_examples_eval.csv',\n",
    "    'gpt_4o_mini_no_examples_eval.csv',\n",
    "    'semantic_similarity_eval.csv',\n",
    "]\n",
    "\n",
    "gpt_4o_paths = [\n",
    "    'gpt_4o_no_examples_eval.csv',\n",
    "    'gpt_4o_with_pseudo_examples_eval.csv',\n",
    "    'gpt_4o_with_examples_eval.csv',\n",
    "    'gpt_4o_generated_examples_eval.csv',\n",
    "    'gpt_4o_overfit_pure_eval.csv',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8a4c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vis import plot_confidence_intervals\n",
    "\n",
    "# Example usage\n",
    "data_dir = 'data/test-results copy'\n",
    "plot_confidence_intervals(\n",
    "    csv_paths=base_result_paths, \n",
    "    output_path='imgs/base-results', \n",
    "    title='Base',\n",
    "    confidence=0.95)\n",
    "\n",
    "plot_confidence_intervals(\n",
    "    csv_paths=gpt_4o_paths, \n",
    "    output_path='imgs/gpt-4o-results', \n",
    "    title='gpt-4o',\n",
    "    confidence=0.95)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4777a6",
   "metadata": {},
   "source": [
    "## What did the models get wrong?\n",
    "Use the following cells to explore the examples that each model struggles with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558a00eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {model_name: result for model_name, result in zip(MODELS_USED, RESULT_DFS)}\n",
    "MODELS_USED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912d378a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = results['gpt_4o_generated_examples']\n",
    "wrong = df[df['classification'] != df['pred']]\n",
    "print(wrong.shape)\n",
    "wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d79145",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dataframe_rows(df):\n",
    "    \"\"\"\n",
    "    Formats each row of the dataframe into a specific string format.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input dataframe with columns:\n",
    "                           - Producer Name_x\n",
    "                           - Producer Name_y\n",
    "                           - Abbreviation Name_x\n",
    "                           - Abbreviation Name_y\n",
    "                           - classification\n",
    "\n",
    "    Returns:\n",
    "        list of str: List of formatted strings for each row in the dataframe.\n",
    "    \"\"\"\n",
    "    formatted_rows = []\n",
    "    for _, row in df.iterrows():\n",
    "        formatted_string = (\n",
    "            \"example_user:\\n\"\n",
    "            \"Are these two names representing the same entity?\\n\"\n",
    "            \"Respond with True or False.\\n\"\n",
    "            f\"Name 1: {row['Producer Name_x']}\\n\"\n",
    "            f\"Name 2: {row['Producer Name_y']}\\n\"\n",
    "            f\"Abbreviation 1: {row['Abbreviation Name_x']}\\n\"\n",
    "            f\"Abbreviation 2: {row['Abbreviation Name_y']}\\n\"\n",
    "            \"example_assistant:\\n\"\n",
    "            f'{{\"is_same_entity\":{bool(row[\"classification\"])}}}'\n",
    "        )\n",
    "        formatted_rows.append(formatted_string)\n",
    "    return formatted_rows\n",
    "\n",
    "examples = format_dataframe_rows(wrong)\n",
    "print(examples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953aa5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in wrong.iterrows():\n",
    "    row = row.to_list()\n",
    "    print(row[1], '|', row[2], '|', row[3], '|', row[4], '|', row[5], '|', row[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78c7cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_4o_mini_no_examples_result = RESULT_DFS[2]\n",
    "wrong = gpt_4o_mini_no_examples_result[gpt_4o_mini_no_examples_result['classification'] != gpt_4o_mini_no_examples_result['pred']]\n",
    "print(wrong.shape)\n",
    "wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b4e2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_35_turbo_no_examples_result = RESULT_DFS[1]\n",
    "wrong = gpt_35_turbo_no_examples_result[gpt_35_turbo_no_examples_result['classification'] != gpt_35_turbo_no_examples_result['pred']]\n",
    "print(wrong.shape)\n",
    "wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c8cca9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.13.0 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "bc72e7bb5a5c6478fda78e4993a878b385c4536c68455efd83492604af2a5a7a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
